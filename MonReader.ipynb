{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>0. Import functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load import check_file_downloaded, extract_zip_file, load_images\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>1. Download ZIP file from Google Drive and unzip in into local drive</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details of the source file in G Drive\n",
    "file_id = \"1KDQBTbo5deKGCdVV_xIujscn5ImxW4dm\"\n",
    "file_url = f\"https://drive.google.com/file/d/{file_id}\"\n",
    "zip_file_name = \"images.zip\"\n",
    "\n",
    "# Details of local directories\n",
    "root_path = os.getcwd()\n",
    "download_path = root_path + \"\\\\\" + \"data\"\n",
    "zip_file_path = download_path + \"\\\\\" + zip_file_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the source file from G Drive if the file does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File images.zip already exists in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(download_path)\n",
    "file_exists = os.path.exists(zip_file_name)\n",
    "if file_exists:\n",
    "    print(f\"File {zip_file_name} already exists in {download_path}.\")\n",
    "else:\n",
    "    print(\"Downloading file from Google Drive.\")\n",
    "    print(\"This could take a few minutes.\")\n",
    "    !gdown 1KDQBTbo5deKGCdVV_xIujscn5ImxW4dm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the downloading was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File images.zip exist in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data!\n"
     ]
    }
   ],
   "source": [
    "check_file_downloaded(file_name=zip_file_name, default_path=root_path, download_path=download_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.zip already extracted in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data.\n"
     ]
    }
   ],
   "source": [
    "extract_zip_file(zip_file_path, download_path, zip_file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>2. Load image files</b>\n",
    "\n",
    "Load images as is without any transformation such as converting to arrays for efficiency and memory saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\n",
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\testing\n",
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\testing\\flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:00<00:00, 1313.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\testing\\notflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:00<00:00, 2249.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\training\n",
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\training\\flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [00:00<00:00, 2411.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\training\\notflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1230/1230 [00:00<00:00, 2300.92it/s]\n"
     ]
    }
   ],
   "source": [
    "array_dict = load_images(download_path, as_array=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1920, 1080, 3)\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "image_shape = None\n",
    "for k, v in array_dict.items():\n",
    "    for k2, v2 in v.items():\n",
    "        for k3, v3 in v2.items():\n",
    "            while image_shape == None:\n",
    "                image_array = asarray(v3)\n",
    "                image_shape = image_array.shape\n",
    "                print(f\"Image shape: {image_shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3. Define a CNN (Convolutional Neural Network)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "    \n",
    "# 0. Initialize a Sequential model from Keras\n",
    "model = Sequential()\n",
    "\n",
    "# 1.  Add a convolutional layer. The first convolutional layer includes an input layer as specified by input_shape.\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=image_shape))\n",
    "\n",
    "# 2. Add a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add another set of convolutional and pooling layers. For this convolutional layer, the number of output filer is 64.\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add another set of convolutional and pooling layers. For this convolutional layer, the number of output filer is 128.\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3. Add a flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# 4. Add a dense (i.e. fully connected) layer with 128 neurons and a ReLU activation function\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# A dropout layer can be added to deal with overfitting.\n",
    "# It will randomly drop 50% of the neurons during training, which helps to reduce overfitting.\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# 5. Add an output layer, which is another dense layer with 1 neurons and a sigmoid activation function\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation of the architecture of the network. Simply put, it is a CNN with multiple convolutional and max pooling layers, followed by a flatten layer, a fully connected layer and a binary classification output layer, which is commonly used for image classification tasks.\n",
    "\n",
    "<b>0. Sequential model</b>\n",
    "\n",
    "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.  This allows us to build a linear stack of layers.\n",
    "\n",
    "<b>1-1. Input layer</b>\n",
    "\n",
    "This layer accepts the input image data, which is typically in the form of a 2D or 3D array, depending on the color channels of the image. In our case, we have 1920 x 1080 RGB pictures so the input_shape would be (1920, 1080, 3).\n",
    "\n",
    "<b>1-2. Convolutional layer</b>\n",
    "\n",
    "This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
    "To put it differently, this layer performs feature extraction by applying a set of filters to the input image. Each filter detects a specific feature, such as edges, corners, or blobs. The output of each filter is a feature map, which highlights the presence of that feature in different parts of the input image.\n",
    "\n",
    "In our CNN, Conv2D from Keras is used, which stands for 2-dimensional convolution.\n",
    "\n",
    "The first parameter of Conv2D (i.e. filters) is the dimensionality of the output space, that is the number of output filters in the convolution. In the code, the first Conv2D layer has 32 filters, the second has 64 filters, and the third has 128 filters. These filters are applied to the input image to extract features that are relevant to the classification task. Increasing the number of filters can help the model learn more complex and abstract features, but also increases the number of parameters in the model, which can make training slower and more computationally intensive.\n",
    "\n",
    "The second parameter (i.e. kernel_size) is the kernel size, specifying the height and width of the 2D convolution window. For binary image classification problems, the typical kernel sizes for the first convolutional layer are in the range of 3x3 to 7x7. Larger kernel sizes may be used for input images with larger spatial dimensions. Smaller kernel sizes can capture fine-grained details in the input image, while larger kernel sizes can capture more global features.\n",
    "\n",
    "The Activation parameter refers to the non-linear function applied to the output of a layer, which adds non-linearity to the model,  allowing it to learn more complex features from the input data. Activation functions are typically applied after the linear transformation of the input data by a layer's weights and biases. This output is then passed through the activation function, which transforms the input into a new output.\n",
    "\n",
    "ReLU (Rectified Linear Unit) is a popular choice for most applications due to its simplicity and effectiveness in reducing the vanishing gradient problem, and sigmoid can be used for binary classification problems. Both activation functions are available in Keras.\n",
    "\n",
    "<b>2. Pooling layer</b>\n",
    "\n",
    "This layer downsamples the feature maps produced by the convolutional layers by taking the maximum or average value within small regions of the feature maps. This helps to reduce the dimensionality of the feature maps and makes the network more computationally efficient.\n",
    "\n",
    "In a Convolutional Neural Network (CNN), pooling layers are commonly used to reduce the spatial dimensions of the input volume (i.e., the height and width dimensions) while preserving the depth dimension. Max pooling and average pooling are two common types of pooling operations used in CNNs.\n",
    "\n",
    "Max pooling takes the maximum value of each non-overlapping rectangular sub-region in the input volume and uses that as the output value for that region. This operation is called \"max\" pooling because it retains the largest (max) value from each region. Max pooling is useful for detecting the presence of a particular feature or pattern in an input volume, as it retains the strongest activation signal in each region.\n",
    "\n",
    "Average pooling takes the average value of each non-overlapping rectangular sub-region in the input volume and uses that as the output value for that region. This operation is called \"average\" pooling because it takes the average value from each region. Average pooling is useful for reducing the spatial dimensions of an input volume while preserving the overall structure of the input, as it retains a more generalized representation of the input volume.\n",
    "\n",
    "In general, max pooling is more commonly used in CNNs because it has been found to work better in practice, especially for tasks like object recognition. However, average pooling can also be useful in some cases, such as for tasks like semantic segmentation where spatial resolution is important.\n",
    "\n",
    "In our CNN, max pooling with a 2x2 pooling window, as specified in the pool_size parameter, is used. This means that the pooling layer will take the max value over a 2x2 pooling window.\n",
    "\n",
    "<b>3. Flatten layer</b>\n",
    "\n",
    "This layer reshapes the output of the previous layers into a 1D array (or one-dimensional vector), which can be fed into a fully connected layer. Without the flatten layer, the output of the final convolutional layer would be a 3D tensor with a fixed spatial structure, which cannot be directly fed into a dense layer (or fully connected layer) that expects a 1D tensor. \n",
    "\n",
    "<b>4. Fully connected (dense) layer</b>\n",
    "\n",
    "This layer performs the final classification by combining the features extracted by the convolutional layers and making a prediction based on them. The output of the final fully connected layer is a probability score indicating the likelihood of the input image belonging to each of the two classes. By fully connected, it means that every neuron in the previous layer is connected to every neuron in the current layer.\n",
    "\n",
    "<b>5. Output layer</b>\n",
    "\n",
    "This layer produces the final binary classification decision based on the probability scores generated by the previous layers. In our code, it is another dense layer with 1 neurons and sigmoid activation function. The sigmoid function squashes the output between 0 and 1, which can be interpreted as the probability of the input image belonging to the positive class.\n",
    "\n",
    "In our CNN, the final layer is another dense layer with a single unit and 'sigmoid' activation function, which outputs the predicted probability of the input belonging to a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://aakashgoel12.medium.com/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "            #   metrics=[Precision(), Recall(), F1Score(num_classes=2)])\n",
    "              metrics=[get_f1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, model.compile() is used to compile the model with a binary cross-entropy loss function, the RMSprop optimizer, and the accuracy metric, which will be used to evaluate the performance of the model during training.\n",
    "\n",
    "* RMSprop stands for Root Mean Square Propagation. It is a gradient descent-based optimization algorithm for neural networks, and it is used to update the weights of the network during training. RMSprop tries to resolve the problems of AdaGrad by using an exponentially decaying average of past gradients.\n",
    "\n",
    "    In RMSprop, the running average of the squared gradient is used to normalize the gradient before updating the weights. This has the effect of scaling down the learning rate for dimensions with high variance and scaling up the learning rate for dimensions with low variance.\n",
    "\n",
    "    RMSprop has been found to be effective in deep learning, particularly in recurrent neural networks, where it has been shown to converge faster than other optimization algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>4. Train the CNN model with train and validation/test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2392 images belonging to 2 classes.\n",
      "Found 597 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = './data/images/training'\n",
    "validation_data_dir = './data/images/testing'\n",
    "nb_train_samples = len(array_dict[\"training\"][\"flip\"]) + len(array_dict[\"training\"][\"notflip\"])\n",
    "nb_validation_samples = len(array_dict[\"testing\"][\"flip\"]) + len(array_dict[\"testing\"][\"notflip\"])\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "\t# rescale=1. / 255,\n",
    "\t# shear_range=0.2,\n",
    "\t# zoom_range=0.2,\n",
    "\t# horizontal_flip=True\n",
    "    )\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    # rescale=1. / 255\n",
    "    )\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "\ttrain_data_dir,\n",
    "\ttarget_size=(img_width_reduced, img_height_reduced),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='binary',\n",
    "\tseed=random.seed(1))\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "\tvalidation_data_dir,\n",
    "\ttarget_size=(img_width_reduced, img_height_reduced),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='binary',\n",
    "\tseed=random.seed(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "74/74 [==============================] - 219s 3s/step - loss: 71.9515 - get_f1: 0.5169 - val_loss: 0.6504 - val_get_f1: 0.6754\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 166s 2s/step - loss: 0.7001 - get_f1: 0.6687 - val_loss: 0.6931 - val_get_f1: 0.6780\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 138s 2s/step - loss: 0.6931 - get_f1: 0.6736 - val_loss: 0.6929 - val_get_f1: 0.6796\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 141s 2s/step - loss: 0.6929 - get_f1: 0.6764 - val_loss: 0.6930 - val_get_f1: 0.6691\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 138s 2s/step - loss: 0.6929 - get_f1: 0.6742 - val_loss: 0.6927 - val_get_f1: 0.6780\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 143s 2s/step - loss: 0.6928 - get_f1: 0.6759 - val_loss: 0.6929 - val_get_f1: 0.6675\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 161s 2s/step - loss: 0.6928 - get_f1: 0.6763 - val_loss: 0.6926 - val_get_f1: 0.6792\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 179s 2s/step - loss: 0.6928 - get_f1: 0.6754 - val_loss: 0.6927 - val_get_f1: 0.6763\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 197s 3s/step - loss: 0.6928 - get_f1: 0.6755 - val_loss: 0.6931 - val_get_f1: 0.6673\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 174s 2s/step - loss: 0.6928 - get_f1: 0.6735 - val_loss: 0.6929 - val_get_f1: 0.6709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x245bc058af0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "\ttrain_generator,\n",
    "\tsteps_per_epoch=nb_train_samples // batch_size,\n",
    "\tepochs=epochs,\n",
    "\tvalidation_data=validation_generator,\n",
    "\tvalidation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step after calling model.fit_generator() with the specified arguments is to wait for the training process to complete. During training, the model will iterate over the training data in batches, compute the gradients, and update the model parameters to minimize the loss. The validation data is also used periodically to evaluate the model performance on unseen data and prevent overfitting.\n",
    "\n",
    "Once the training is complete, you can use the model.evaluate() method to compute the final loss and accuracy on the validation set, or use the model.predict() method to make predictions on new data. You can also save the trained model to disk using the model.save() method, so that you can reload it later and use it to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 22s 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       290\n",
      "         1.0       0.51      1.00      0.68       307\n",
      "\n",
      "    accuracy                           0.51       597\n",
      "   macro avg       0.26      0.50      0.34       597\n",
      "weighted avg       0.26      0.51      0.35       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(validation_generator)\n",
    "\n",
    "# convert predictions from probabilities to labels\n",
    "y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# print the classification report containing precision, recall and F1 score\n",
    "y_true = []\n",
    "for i in range(len(validation_generator)):\n",
    "    _, labels = validation_generator[i]\n",
    "    y_true.extend(labels)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 17s 904ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3396017699115044"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(validation_generator)\n",
    "y_pred = y_pred.round()\n",
    "\n",
    "# Extract true labels\n",
    "y_true = []\n",
    "for i in range(len(validation_generator)):\n",
    "    _, labels = validation_generator[i]\n",
    "    y_true.extend(labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 14s 728ms/step - loss: 0.6928 - get_f1: 0.6706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6927505135536194, 0.6705861687660217]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# from keras.preprocessing.image import load_img\n",
    "# from keras.preprocessing.image import img_to_array\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# from keras.applications.vgg16 import decode_predictions\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# import numpy as np\n",
    "\n",
    "# from keras.models import load_model\n",
    "\n",
    "# model = load_model('model_saved.h5')\n",
    "\n",
    "# image = load_img('v_data/test/planes/5.jpg', target_size=(224, 224))\n",
    "# img = np.array(image)\n",
    "# img = img / 255.0\n",
    "# img = img.reshape(1,224,224,3)\n",
    "# label = model.predict(img)\n",
    "# print(\"Predicted Class (0 - Cars , 1- Planes): \", label[0][0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/python-image-classification-using-keras/\n",
    "\n",
    "https://medium.com/techiepedia/binary-image-classifier-cnn-using-tensorflow-a3f5d6746697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Python program to create\n",
    "# # # Image Classifier using CNN\n",
    "\n",
    "# # # Importing the required libraries\n",
    "# # import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from random import shuffle\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # '''Setting up the env'''\n",
    "\n",
    "# # TRAIN_DIR = 'E:/dataset / Cats_vs_Dogs / train'\n",
    "# # TEST_DIR = 'E:/dataset / Cats_vs_Dogs / test1'\n",
    "# # IMG_SIZE = 50\n",
    "# LR = 1e-3\n",
    "\n",
    "\n",
    "# # '''Setting up the model which will help with tensorflow models'''\n",
    "# # MODEL_NAME = 'dogsvscats-{}-{}.model'.format(LR, '6conv-basic')\n",
    "\n",
    "# # '''Labelling the dataset'''\n",
    "# # def label_img(img):\n",
    "# # \tword_label = img.split('.')[-3]\n",
    "# # \t# DIY One hot encoder\n",
    "# # \tif word_label == 'cat': return [1, 0]\n",
    "# # \telif word_label == 'dog': return [0, 1]\n",
    "\n",
    "# # '''Creating the training data'''\n",
    "# # def create_train_data():\n",
    "# # \t# Creating an empty list where we should store the training data\n",
    "# # \t# after a little preprocessing of the data\n",
    "# # \ttraining_data = []\n",
    "\n",
    "# # \t# tqdm is only used for interactive loading\n",
    "# # \t# loading the training data\n",
    "# # \tfor img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "\n",
    "# # \t\t# labeling the images\n",
    "# # \t\tlabel = label_img(img)\n",
    "\n",
    "# # \t\tpath = os.path.join(TRAIN_DIR, img)\n",
    "\n",
    "# # \t\t# loading the image from the path and then converting them into\n",
    "# # \t\t# grayscale for easier covnet prob\n",
    "# # \t\timg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# # \t\t# resizing the image for processing them in the covnet\n",
    "# # \t\timg = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "# # \t\t# final step-forming the training data list with numpy array of the images\n",
    "# # \t\ttraining_data.append([np.array(img), np.array(label)])\n",
    "\n",
    "# # \t# shuffling of the training data to preserve the random state of our data\n",
    "# # \tshuffle(training_data)\n",
    "\n",
    "# # \t# saving our trained data for further uses if required\n",
    "# # \tnp.save('train_data.npy', training_data)\n",
    "# # \treturn training_data\n",
    "\n",
    "# # '''Processing the given test data'''\n",
    "# # # Almost same as processing the training data but\n",
    "# # # we dont have to label it.\n",
    "# # def process_test_data():\n",
    "# # \ttesting_data = []\n",
    "# # \tfor img in tqdm(os.listdir(TEST_DIR)):\n",
    "# # \t\tpath = os.path.join(TEST_DIR, img)\n",
    "# # \t\timg_num = img.split('.')[0]\n",
    "# # \t\timg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "# # \t\timg = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "# # \t\ttesting_data.append([np.array(img), img_num])\n",
    "\t\t\n",
    "# # \tshuffle(testing_data)\n",
    "# # \tnp.save('test_data.npy', testing_data)\n",
    "# # \treturn testing_data\n",
    "\n",
    "# # '''Running the training and the testing in the dataset for our model'''\n",
    "# # train_data = create_train_data()\n",
    "# # test_data = process_test_data()\n",
    "\n",
    "# # # train_data = np.load('train_data.npy')\n",
    "# # # test_data = np.load('test_data.npy')\n",
    "# '''Creating the neural network using tensorflow'''\n",
    "# # Importing the required libraries\n",
    "# import tflearn\n",
    "# from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "# from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "# from tflearn.layers.estimator import regression\n",
    "\n",
    "# import tensorflow as tf\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# convnet = input_data(shape =[None, 1920, 1080, 3], name ='input')\n",
    "\n",
    "# convnet = conv_2d(convnet, 32, 5, activation ='relu')\n",
    "# convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# # convnet = conv_2d(convnet, 64, 5, activation ='relu')\n",
    "# # convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# # convnet = conv_2d(convnet, 128, 5, activation ='relu')\n",
    "# # convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# # convnet = conv_2d(convnet, 64, 5, activation ='relu')\n",
    "# # convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# # convnet = conv_2d(convnet, 32, 5, activation ='relu')\n",
    "# # convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# convnet = fully_connected(convnet, 1024, activation ='relu')\n",
    "# convnet = dropout(convnet, 0.8)\n",
    "\n",
    "# convnet = fully_connected(convnet, 2, activation ='softmax')\n",
    "# convnet = regression(convnet, optimizer ='adam', learning_rate = LR,\n",
    "# \tloss ='categorical_crossentropy', name ='targets')\n",
    "\n",
    "# model = tflearn.DNN(convnet, tensorboard_dir ='log')\n",
    "\n",
    "# # Splitting the testing data and training data\n",
    "# # train = train_data[:-500]\n",
    "# # test = train_data[-500:]\n",
    "\n",
    "# '''Setting up the features and labels'''\n",
    "# # X-Features & Y-Labels\n",
    "\n",
    "# train_X = np.array([i[0] for i in train_data]).reshape(-1, 1920, 1080, 3)\n",
    "# train_y = np.array([i[1] for i in train_data])\n",
    "# test_X = np.array([i[0] for i in test_data]).reshape(-1, 1920, 1080, 3)\n",
    "# test_y = np.array([i[1] for i in test_data])\n",
    "\n",
    "# '''Fitting the data into our model'''\n",
    "# # epoch = 5 taken\n",
    "# model.fit({'input': train_X}, {'targets': train_y}, n_epoch = 5,\n",
    "# \tvalidation_set =({'input': test_X}, {'targets': test_y}),\n",
    "# \tsnapshot_step = 500, show_metric = True, run_id = \"initial.model\")\n",
    "# model.save(\"initial.model\")\n",
    "\n",
    "# '''Testing the data'''\n",
    "# import matplotlib.pyplot as plt\n",
    "# # if you need to create the data:\n",
    "# # test_data = process_test_data()\n",
    "# # if you already have some saved:\n",
    "# test_data = np.load('test_data.npy')\n",
    "\n",
    "# fig = plt.figure()\n",
    "\n",
    "# for num, data in enumerate(test_data[:20]):\n",
    "# \t# cat: [1, 0]\n",
    "# \t# dog: [0, 1]\n",
    "\t\n",
    "# \timg_num = data[1]\n",
    "# \timg_data = data[0]\n",
    "\t\n",
    "# \ty = fig.add_subplot(4, 5, num + 1)\n",
    "# \torig = img_data\n",
    "# \tdata = img_data.reshape(1920, 1080, 3)\n",
    "\n",
    "# \t# model_out = model.predict([data])[0]\n",
    "# \tmodel_out = model.predict([data])[0]\n",
    "\t\n",
    "# \tif np.argmax(model_out) == 1: str_label ='Dog'\n",
    "# \telse: str_label ='Cat'\n",
    "\t\t\n",
    "# \ty.imshow(orig, cmap ='gray')\n",
    "# \tplt.title(str_label)\n",
    "# \ty.axes.get_xaxis().set_visible(False)\n",
    "# \ty.axes.get_yaxis().set_visible(False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
