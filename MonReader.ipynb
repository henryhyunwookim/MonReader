{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Table of Content</b>\n",
    "\n",
    "0. Import functions\n",
    "\n",
    "1. Download ZIP file from Google Drive and unzip in into local drive\n",
    "\n",
    "2. Load image files\n",
    "\n",
    "3. Define a CNN (Convolutional Neural Network)\n",
    "\n",
    "    3-1.Initialize a Sequential model from Keras and add layers to it\n",
    "\n",
    "    3-2. Compile the model with the f1 score as the evaluation metric\n",
    "\n",
    "4. Train the CNN model and evaluate model performance\n",
    "\n",
    "5. Save models for later use\n",
    "\n",
    "6. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>0. Import functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load import extract_zip_file, load_images, get_image_shape\n",
    "from utils.stats_functions import f1_score\n",
    "from utils.evaluate import return_result_table\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.utils import image_dataset_from_directory, custom_object_scope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>1. Download ZIP file from Google Drive and unzip in into local drive</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details of the source file in G Drive\n",
    "file_id = \"1KDQBTbo5deKGCdVV_xIujscn5ImxW4dm\"\n",
    "file_url = f\"https://drive.google.com/file/d/{file_id}\"\n",
    "zip_file_name = \"images.zip\"\n",
    "\n",
    "# Details of local directories\n",
    "root_path = sys.path[0]\n",
    "download_path = root_path + \"\\\\\" + \"data\"\n",
    "zip_file_path = download_path + \"\\\\\" + zip_file_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the source file from G Drive if the file does not already exist in the download path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File images.zip already exists in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(zip_file_path):\n",
    "    print(f\"File {zip_file_name} already exists in {download_path}.\")\n",
    "else:\n",
    "    print(\"Downloading file from Google Drive.\")\n",
    "    print(\"This could take a few minutes.\")\n",
    "    !gdown 1KDQBTbo5deKGCdVV_xIujscn5ImxW4dm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.zip already extracted in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data.\n"
     ]
    }
   ],
   "source": [
    "extract_zip_file(zip_file_path, download_path, zip_file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>2. Load image files</b>\n",
    "\n",
    "Load images as is without any transformation such as converting to arrays for efficiency and less memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\n",
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\testing\n",
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\testing\\flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:00<00:00, 753.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\testing\\notflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:00<00:00, 859.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\training\n",
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\training\\flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [00:00<00:00, 1344.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\lnaNWaYIRf6JhvHJ\\data\\images\\training\\notflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1230/1230 [00:00<00:00, 2861.05it/s]\n"
     ]
    }
   ],
   "source": [
    "array_dict = load_images(download_path, as_array=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1920, 1080, 3)\n"
     ]
    }
   ],
   "source": [
    "image_shape = get_image_shape(array_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3. Define a CNN (Convolutional Neural Network)</b>\n",
    "\n",
    "##### 3-1.Initialize a Sequential model from Keras and add layers to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 186, 102, 8)       1184      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 62, 34, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 56, 28, 16)        6288      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 18, 9, 16)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 3, 64)         50240     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 384)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                12320     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,065\n",
      "Trainable params: 70,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 0. Initialize a Sequential model from Keras\n",
    "model = Sequential()\n",
    "\n",
    "# 1.  Add a convolutional layer. The first convolutional layer includes an input layer as specified by input_shape.\n",
    "reduced_image_shape = (int(image_shape[0]/10), int(image_shape[1]/10), image_shape[2])\n",
    "model.add(Conv2D(filters=8, kernel_size=(7, 7), activation='relu', input_shape=reduced_image_shape))\n",
    "\n",
    "# 2. Add a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "# Add another set of convolutional (with a different number of output filters) and pooling layers\n",
    "model.add(Conv2D(filters=16, kernel_size=(7, 7), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "# Add another set of convolutional (with a different number of output filters) and pooling layers\n",
    "model.add(Conv2D(filters=64, kernel_size=(7, 7), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3. Add a flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# 4. Add a dense (i.e. fully connected) layer with 32 neurons and a ReLU activation function\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "# A dropout layer can be added to deal with overfitting;\n",
    "# The below line of code will randomly drop 50% of the neurons during training, which helps to reduce overfitting\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# 5. Add an output layer, which is another dense layer with 1 neurons and a sigmoid activation function\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Print out the summary of the model\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation of the architecture of the network. Simply put, it is a CNN with multiple convolutional and max pooling layers, followed by a flatten layer, a fully connected layer and a binary classification output layer. This architecture is commonly used for image classification tasks.\n",
    "\n",
    "<b>0. Sequential model</b>\n",
    "\n",
    "A Sequential model allows us to build a linear stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "<b>1-1. Input layer</b>\n",
    "\n",
    "Input layers accept input image data, which is typically in the form of a 2D or 3D array, depending on the color channels of the image. In our case, we have 1920 x 1080 RGB pictures so the input shape would be (1920, 1080, 3). To reduce the training time and memory use, we reduced the first two dimensions of the input shape into one-tenth of the original shape.\n",
    "\n",
    "<b>1-2. Convolutional layer</b>\n",
    "\n",
    "Convolutional layers perform feature extraction by applying a set of filters to the input image. Each filter detects a specific feature, such as edges, corners, blobs, etc. The output of each filter is a feature map, which highlights the presence of that feature in different parts of the input image.\n",
    "\n",
    "In our CNN, Conv2D from Keras is used, which stands for 2-dimensional convolution.\n",
    "\n",
    "<b>Output filters</b>\n",
    "\n",
    "The first parameter of Conv2D (i.e., filters) is the dimensionality of the output space, that is, the number of output filters in the convolution. In the code, the three Conv2D layers have 8, 16, and 64 filters, respectively. These filters are applied to the input image to extract features that are relevant to the classification task. Increasing the number of filters can help the model learn more complex and abstract features, but that will increase the number of parameters in the model, making training slower and more computationally intensive.\n",
    "\n",
    "It is generally better to have a different number of filters in different convolutional layers in a CNN. In the earlier layers of the network, it is common to use a small number of filters, such as 32 or 64, to extract simple and general features from the input images. In the later layers of the network, a larger number of filters, such as 128 or 256, are often used to extract more complex and specific features.\n",
    "\n",
    "Using different numbers of filters in different convolutional layers can help the model learn more efficiently and effectively. It allows the network to identify simple and general features in the early layers, and then build on those features with more complex and specific features in the later layers. Additionally, using fewer filters in the early layers can help to reduce the number of parameters in the network, which can help to prevent overfitting.\n",
    "\n",
    "<b>Kernel size</b>\n",
    "\n",
    "The second parameter (i.e. kernel_size) is the kernel size, specifying the height and width of the 2D convolution window. For binary image classification problems, the typical kernel sizes for the first convolutional layer are in the range of 3x3 to 7x7. Larger kernel sizes may be used for input images with larger spatial dimensions. Smaller kernel sizes can capture fine-grained details in the input image, while larger kernel sizes can capture more global features.\n",
    "\n",
    "<b>Activation function</b>\n",
    "\n",
    "The Activation parameter refers to the non-linear function applied to the output of a layer, which adds non-linearity to the model,  allowing it to learn more complex features from the input data. Activation functions are typically applied after the linear transformation of the input data by a layer's weights and biases. This output is then passed through the activation function, which transforms the input into a new output.\n",
    "\n",
    "ReLU (Rectified Linear Unit) is a popular choice for most applications due to its simplicity and effectiveness in reducing the vanishing gradient problem, and sigmoid can be used for binary classification problems. Both activation functions are available in Keras and are used in our code.\n",
    "\n",
    "<b>2. Pooling layer</b>\n",
    "\n",
    "Pooling layers downsample the feature maps produced by the convolutional layers, commonly by taking the maximum or average value (i.e., max pooling or average pooling) within small regions of the feature maps. This helps reduce the dimensionality of the feature maps (i.e., the height and width dimensions while preserving the depth dimension) and makes the network more computationally efficient.\n",
    "\n",
    "Max pooling takes the maximum value of each non-overlapping rectangular sub-region in the input volume and uses that as the output value for that region. This operation is called \"max\" pooling because it retains the largest (max) value from each region. Max pooling is useful for detecting the presence of a particular feature or pattern in an input volume, as it retains the strongest activation signal in each region.\n",
    "\n",
    "Average pooling takes the average value of each non-overlapping rectangular sub-region in the input volume and uses that as the output value for that region. This operation is called \"average\" pooling because it takes the average value from each region. Average pooling is useful for reducing the spatial dimensions of an input volume while preserving the overall structure of the input, as it retains a more generalized representation of the input volume.\n",
    "\n",
    "In general, max pooling is more commonly used in CNNs because it has been found to work better in practice, especially for tasks like object recognition. However, average pooling can also be useful in some cases, such as for tasks like semantic segmentation where spatial resolution is important.\n",
    "\n",
    "In our CNN, max pooling with a 3x3 (or 2x2) pooling window, as specified in the pool_size parameter, is used for each pooling layer. This means that the pooling layers will take the max value over a 3x3 (or 2x2) pooling window.\n",
    "\n",
    "<b>3. Flatten layer</b>\n",
    "\n",
    "Flatten layers reshape the output of the previous layers into a 1D array (or one-dimensional vector), which can be fed into a fully connected layer. Without a flatten layer, the output of the final convolutional layer would be a 3D tensor with a fixed spatial structure, which cannot be directly fed into a dense (or fully connected) layer that expects a 1D tensor.\n",
    "\n",
    "<b>4. Dense (or fully connected) layer</b>\n",
    "\n",
    "Dense layers perform the final classification by combining the features extracted by the convolutional layers and making a prediction based on them. The output of the final fully connected layer is a probability score indicating the likelihood of the input image belonging to each of the two classes. By fully connected, it means that every neuron in the previous layer is connected to every neuron in the current layer.\n",
    "\n",
    "<b>5. Output layer</b>\n",
    "\n",
    "This layer produces the final binary classification decision based on the probability scores generated by the previous layers. In our CNN, it is another dense layer with a single neuron and a sigmoid activation function. The sigmoid function squashes the output between 0 and 1, which can be interpreted as the probability of the input image belonging to the positive class. That is, the output of the output layer would be the predicted probability of each input image belonging to a certain class, in our case either 'flip' or 'notflip'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "##### 3-2. Compile the model with the f1 score as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[f1_score]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary cross-entropy is the most commonly used loss function for binary image classification tasks, where the output of the model is a probability distribution over two classes (i.e., flip or notflip in our case). Binary cross-entropy measures the difference between the predicted and true labels for each binary classification instance.\n",
    "\n",
    "Adam is a popular optimizer that is often used for binary classification problems. It is an adaptive learning rate optimization algorithm that is well-suited for large datasets and high-dimensional parameter spaces.\n",
    "\n",
    "For evaluating model performance during training and testing, we use the f1 score since it's the success metric of the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>4. Train the CNN model and evaluate model performance</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2392 files belonging to 2 classes.\n",
      "Using 1914 files for training.\n",
      "Using 478 files for validation.\n",
      "Found 597 files belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "30/30 - 47s - loss: 0.3860 - f1_score: 0.8482 - val_loss: 0.3624 - val_f1_score: 0.8529 - 47s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "30/30 - 50s - loss: 0.2705 - f1_score: 0.9060 - val_loss: 0.4459 - val_f1_score: 0.7492 - 50s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "30/30 - 45s - loss: 0.1978 - f1_score: 0.9323 - val_loss: 0.2013 - val_f1_score: 0.9338 - 45s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "30/30 - 45s - loss: 0.1326 - f1_score: 0.9566 - val_loss: 0.1452 - val_f1_score: 0.9552 - 45s/epoch - 1s/step\n",
      "Epoch 5/10\n",
      "30/30 - 45s - loss: 0.0950 - f1_score: 0.9710 - val_loss: 0.1274 - val_f1_score: 0.9505 - 45s/epoch - 1s/step\n",
      "Epoch 6/10\n",
      "30/30 - 43s - loss: 0.0738 - f1_score: 0.9812 - val_loss: 0.1014 - val_f1_score: 0.9677 - 43s/epoch - 1s/step\n",
      "Epoch 7/10\n",
      "30/30 - 38s - loss: 0.0533 - f1_score: 0.9892 - val_loss: 0.1080 - val_f1_score: 0.9637 - 38s/epoch - 1s/step\n",
      "Epoch 8/10\n",
      "30/30 - 53s - loss: 0.0387 - f1_score: 0.9937 - val_loss: 0.0638 - val_f1_score: 0.9790 - 53s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "30/30 - 49s - loss: 0.0233 - f1_score: 0.9958 - val_loss: 0.0616 - val_f1_score: 0.9761 - 49s/epoch - 2s/step\n",
      "Epoch 10/10\n",
      "30/30 - 38s - loss: 0.0316 - f1_score: 0.9931 - val_loss: 0.0810 - val_f1_score: 0.9834 - 38s/epoch - 1s/step\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = './data/images/training'\n",
    "test_data_dir = './data/images/testing'\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "train_data, validate_data = image_dataset_from_directory(\n",
    "    directory=train_data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    image_size=reduced_image_shape[:2], # (height, width) to resize images to after they are read from disk\n",
    "    shuffle=True,\n",
    "    seed=1,\n",
    "\n",
    "    # If True, resize the images without aspect ratio distortion.\n",
    "    # When the original aspect ratio differs from the target aspect ratio,\n",
    "    # the output image will be cropped so as to return the largest possible window \n",
    "    # in the image (of size `image_size`) that matches the target aspect ratio.\n",
    "    # By default (i.e., 'crop_to_aspect_ratio=False'), aspect ratio may not be preserved.\n",
    "    crop_to_aspect_ratio=True,\n",
    "    \n",
    "    validation_split=0.2, # 20% of the data will be reserved for validation\n",
    "    subset='both', # Subset of the data to return. 'both' returns a tuple of the training and validation datasets.\n",
    ")\n",
    "\n",
    "test_data = image_dataset_from_directory(\n",
    "    directory=test_data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    image_size=reduced_image_shape[:2],\n",
    "    shuffle=True,\n",
    "    seed=1,\n",
    "    crop_to_aspect_ratio=True\n",
    ")\n",
    "\n",
    "results = model.fit(\n",
    "\n",
    "    # Since we pass a generator (i.e., train_data) to 'x',\n",
    "    # 'y' should not be specified - targets will be obtained from 'x'.\n",
    "    x=train_data,\n",
    "\n",
    "    epochs=epochs,\n",
    "    verbose=2, # This will output one line per epoch\n",
    "    validation_data=validate_data\n",
    "    \n",
    "    # This argument is not supported when 'x' is a dataset, generator or 'keras.utils.Sequence' instance\n",
    "    # validation_split=0.2\n",
    "\n",
    "    # This is not required when the data is in the form of generators since they generate batches\n",
    "    # # batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size in a Convolutional Neural Network (CNN) refers to the number of images that are processed in a single forward/backward pass. The choice of batch size can impact the performance of the model, as well as the training time and memory requirements. In general, batch sizes between 32 and 128 are commonly used for CNN models for image classification.\n",
    "\n",
    "Here are some important features of smaller and larger batch sizes.\n",
    "- Smaller batch size: Less memory usage, suitable for small data, a model with a large number of parameters, or a very deep model to prevent overfitting\n",
    "- Larger batch size: faster training, can train large data or a relatively simple model without overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training (through .fit method), the model will iterate over the training data in batches, compute the gradients, and update the model parameters to minimize the loss. The validation data is also used periodically to evaluate the model performance on unseen data and to prevent overfitting.\n",
    "\n",
    "Once the training is complete, we can use the .evaluate method to compute the final loss and f1 score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 6s 206ms/step - loss: 0.0722 - f1_score: 0.9813\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = model.evaluate(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a summary table of model performance, in terms of the loss and f1 score on different sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Validate loss</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>Train f1</th>\n",
       "      <th>Validate f1</th>\n",
       "      <th>Test f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.1303</td>\n",
       "      <td>0.1698</td>\n",
       "      <td>0.0722</td>\n",
       "      <td>0.9567</td>\n",
       "      <td>0.9312</td>\n",
       "      <td>0.9813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.1138</td>\n",
       "      <td>0.1249</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0704</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>0.3860</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.0722</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.0722</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7492</td>\n",
       "      <td>0.9813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train loss  Validate loss  Test loss  Train f1  Validate f1  Test f1\n",
       "Mean      0.1303         0.1698     0.0722    0.9567       0.9312   0.9813\n",
       "Std       0.1138         0.1249     0.0000    0.0458       0.0704   0.0000\n",
       "Max       0.3860         0.4459     0.0722    0.9958       0.9834   0.9813\n",
       "Min       0.0233         0.0616     0.0722    0.8482       0.7492   0.9813"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = results.history['loss']\n",
    "train_f1 = results.history['f1_score']\n",
    "val_loss = results.history['val_loss']\n",
    "val_f1 = results.history['val_f1_score']\n",
    "return_result_table(results = [train_loss, val_loss, test_loss, train_f1, val_f1, test_f1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>5. Save model for later use</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the previous model if exists in local drive, and evaluate its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('mon_reader_model.h5'):\n",
    "    with custom_object_scope({'f1_score': f1_score}):\n",
    "        loaded_model = load_model('mon_reader_model.h5')\n",
    "\n",
    "loaded_model_loss, loaded_model_test_f1 = loaded_model.evaluate(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model if no model exists in local drive.\n",
    "\n",
    "If the previous model exists, compare the performance between the previous and current models. Save the current model only if it performed better than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newly trained model didn't perform better than the previous model.\n",
      "Previous f1 score: 0.9839, New f1 score: 0.9813\n",
      "Not saving the current model.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('mon_reader_model.h5'):\n",
    "    print('No model exists in local drive.')\n",
    "    model.save('mon_reader_model.h5')\n",
    "    print('Current model saved.')\n",
    "\n",
    "elif test_f1 > loaded_model_test_f1:\n",
    "    print('The newly trained model performed better than the previous model.')\n",
    "    print(f'Previous f1 score: {round(loaded_model_test_f1, 4)}, New f1 score: {round(test_f1, 4)}')\n",
    "    model.save('mon_reader_model.h5')\n",
    "    print('Current model saved.')\n",
    "\n",
    "else:\n",
    "    print('The newly trained model didn\\'t perform better than the previous model.')\n",
    "    print(f'Previous f1 score: {round(loaded_model_test_f1, 4)}, New f1 score: {round(test_f1, 4)}')\n",
    "    print('Not saving the current model.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>6. Conclusion</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By experimenting with different input shapes, number of filters, kernel and pool sizes in the CNN model, we were able to construct a highly efficient and performant CNN model using more than two thousand high-resolution images (1920 x 1080 x 3). Fitting the model on the training data took less than 10 minutes, yet the performance was promising. The best model returned an f1 score of 0.9839, which is close to the perfect score of 1.0. The model is saved for later use, i.e., for similar image classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
